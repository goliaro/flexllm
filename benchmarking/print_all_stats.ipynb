{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, namedtuple\n",
    "import os, itertools, json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def get_tpot(df_original):\n",
    "    df = df_original.copy()\n",
    "    # remove entries where is_warmup_request is 1 or decoding_step_idx is < 0\n",
    "    df = df[(df[\"is_warmup_request\"] == 0) & (df[\"decoding_step_idx\"] >= 0)]\n",
    "    group = df.groupby(\"request_guid\", as_index=False)\n",
    "    min_time = group[\"timestamp\"].min()[\"timestamp\"]\n",
    "    max_time = group[\"timestamp\"].max()[\"timestamp\"]\n",
    "    num_generated_tokens = group.size()[\"size\"]\n",
    "    tpots = (max_time - min_time) / num_generated_tokens / 1000\n",
    "    # return mean and p99 of tpots\n",
    "    return tpots.mean(), tpots.median(), tpots.quantile(0.99)\n",
    "\n",
    "def get_throughput(df_original):\n",
    "    df = df_original.copy()\n",
    "    # remove entries where is_warmup_request is 1 or request_step_idx is < 0\n",
    "    df = df[(df[\"is_warmup_request\"] == 0) & (df[\"decoding_step_idx\"] >= 0)]\n",
    "    # compute the throughput as the number of rows in the filtered dataframe (df) divided by the total time taken\n",
    "    microsec_to_sec = 1_000_000\n",
    "    total_time_sec = (df[\"timestamp\"].max() - df[\"timestamp\"].min()) / microsec_to_sec\n",
    "    total_output_tokens = df.shape[0]\n",
    "    return total_output_tokens / total_time_sec\n",
    "def get_ttft(df_original):\n",
    "    df = df_original.copy()\n",
    "    # remove entries where is_warmup_request is 1\n",
    "    df = df[(df[\"is_warmup_request\"] == 0)]\n",
    "    group = df.groupby(\"request_guid\", as_index=False)\n",
    "    ttft = group.apply(lambda x: x[x[\"decoding_step_idx\"] == 0][\"timestamp\"].values[0] - x[x[\"decoding_step_idx\"] == -1][\"timestamp\"].values[0])/1000\n",
    "    # convert to milliseconds from microseconds\n",
    "    return ttft.mean().iloc[1], ttft.median().iloc[1], ttft.quantile(0.99).iloc[1]\n",
    "\n",
    "def get_queueing_time(df_original):\n",
    "    df = df_original.copy()\n",
    "    # remove entries where is_warmup_request is 1\n",
    "    df = df[(df[\"is_warmup_request\"] == 0)]\n",
    "    group = df.groupby(\"request_guid\", as_index=False)\n",
    "    microsec_to_sec = 1_000_000\n",
    "    # in each group, find the difference between the timestampt at request_step_idx=-1 and the timestamp at request_step_idx=-2.\n",
    "    queueing_time = group.apply(lambda x: x[x[\"decoding_step_idx\"] == -1][\"timestamp\"].values[0] - x[x[\"decoding_step_idx\"] == -2][\"timestamp\"].values[0])/1000\n",
    "    return queueing_time.mean().iloc[1], queueing_time.median().iloc[1], queueing_time.quantile(0.99).iloc[1]\n",
    "def get_ft_throughput(df_original):\n",
    "    df = df_original.copy()\n",
    "    # remove entries where is_warmup_request is 1 or request_step_idx is < 0\n",
    "    df = df[df[\"is_warmup_step\"] == 0]\n",
    "    # compute the throughput as the number of rows in the filtered dataframe (df) divided by the total time taken\n",
    "    microsec_to_sec = 1_000_000\n",
    "    total_time_sec = (df[\"timestamp\"].max() - df[\"timestamp\"].min()) / microsec_to_sec\n",
    "    total_output_tokens = df[\"num_finetuning_fwd_tokens\"].sum()\n",
    "    return total_output_tokens / total_time_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, TP Degree: 1, KV Cache Slots: 70000\n",
      "QPS: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "TPOT - Mean: ['32.567', '33.594', '34.854', '36.053', '36.674'], P99: ['42.962', '43.232', '45.365', '45.409', '48.638']\n",
      "Inf Throughput: ['259.807', '513.973', '751.252', '1059.269', '1307.249']\n",
      "Queueing Time - Mean: ['40.472', '77.131', '104.183', '139.490', '181.138'], P99: ['335.125', '678.591', '883.131', '965.131', '1210.971']\n",
      "TTFT - Mean: ['68.906', '76.272', '81.292', '85.394', '87.066'], P99: ['453.013', '598.573', '639.797', '621.888', '617.447']\n",
      "Finetuning Throughput: ['2351.923', '2228.795', '2093.183', '1937.874', '1798.804']\n",
      "--------------------------------------------------\n",
      "Model: Qwen/Qwen2.5-14B-Instruct, TP Degree: 2, KV Cache Slots: 70000\n",
      "QPS: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "TPOT - Mean: ['57.926', '60.943', '64.268', '66.207', '66.455'], P99: ['66.487', '69.214', '72.123', '75.286', '73.150']\n",
      "Inf Throughput: ['258.493', '504.365', '604.072', '887.988', '1178.928']\n",
      "Queueing Time - Mean: ['64.015', '135.692', '241.853', '399.249', '1042.903'], P99: ['633.852', '1250.286', '2152.932', '2711.966', '6698.118']\n",
      "TTFT - Mean: ['117.704', '133.967', '147.121', '160.080', '169.586'], P99: ['728.462', '932.956', '1005.813', '1239.433', '1315.303']\n",
      "Finetuning Throughput: ['941.164', '857.023', '812.573', '700.685', '554.689']\n",
      "--------------------------------------------------\n",
      "Model: Qwen/Qwen2.5-32B-Instruct, TP Degree: 4, KV Cache Slots: 60000\n",
      "QPS: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "TPOT - Mean: ['53.761', '55.274', '56.932', '58.570', '59.806'], P99: ['62.352', '63.886', '66.175', '68.457', '69.608']\n",
      "Inf Throughput: ['263.231', '513.932', '635.913', '951.413', '1267.362']\n",
      "Queueing Time - Mean: ['136.623', '208.557', '292.192', '406.545', '1551.717'], P99: ['1051.481', '5194.696', '3874.296', '4290.712', '9570.397']\n",
      "TTFT - Mean: ['111.456', '120.354', '130.648', '141.450', '149.590'], P99: ['709.494', '879.288', '957.383', '1018.972', '1080.418']\n",
      "Finetuning Throughput: ['802.811', '759.636', '734.086', '655.820', '559.008']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "directory = \"/global/homes/g/goliaro/flexllm/benchmarking/output/e2e/coserving/profiling\"\n",
    "models=[\"meta-llama/Llama-3.1-8B-Instruct\", \"Qwen/Qwen2.5-14B-Instruct\", \"Qwen/Qwen2.5-32B-Instruct\"]\n",
    "tp_degrees=[1, 2, 4]\n",
    "kv_cache_slots_values=[70000, 70000, 60000]\n",
    "qps_values=[\"1.0\",\"2.0\",\"3.0\",\"4.0\",\"5.0\"]\n",
    "for i, model in enumerate(models):\n",
    "    model_ = model.replace(\"/\", \"_\").lower()\n",
    "    tp_degree = tp_degrees[i]\n",
    "    kv_cache_slots = kv_cache_slots_values[i]\n",
    "    mean_tpots=[]\n",
    "    p99_tpots=[]\n",
    "    inf_throughputs=[]\n",
    "    mean_queue_times=[]\n",
    "    p99_queue_times=[]\n",
    "    mean_ttfts=[]\n",
    "    p99_ttfts=[]\n",
    "    ft_throughputs=[]\n",
    "    for qps in qps_values:\n",
    "        filepath= os.path.join(directory, f\"inference_request_profiling_sharegpt_8192_{qps}_qps_{model_}_tensor_parallelism_{tp_degree}_max_requests_per_batch_256_max_tokens_per_batch_256_num_kv_cache_slots_{kv_cache_slots}_qps_0.000000_num_warmup_requests_0.csv\")\n",
    "        if os.path.exists(filepath):\n",
    "            df = pd.read_csv(filepath)\n",
    "            tpot_mean, tpot_median, tpot_p99 = get_tpot(df)\n",
    "            throughput = get_throughput(df)\n",
    "            queueing_time = get_queueing_time(df)\n",
    "            ttft = get_ttft(df)\n",
    "            mean_tpots.append(f\"{tpot_mean:.3f}\")\n",
    "            p99_tpots.append(f\"{tpot_p99:.3f}\")\n",
    "            inf_throughputs.append(f\"{throughput:.3f}\")\n",
    "            mean_queue_times.append(f\"{queueing_time[0]:.3f}\")\n",
    "            p99_queue_times.append(f\"{queueing_time[2]:.3f}\")\n",
    "            mean_ttfts.append(f\"{ttft[0]:.3f}\")\n",
    "            p99_ttfts.append(f\"{ttft[2]:.3f}\")\n",
    "        else:\n",
    "            print(f\"File {filepath} does not exist.\")\n",
    "            mean_tpots.append(f\"{0:.3f}\")\n",
    "            p99_tpots.append(f\"{0:.3f}\")\n",
    "            inf_throughputs.append(f\"{0:.3f}\")\n",
    "            mean_queue_times.append(f\"{0:.3f}\")\n",
    "            p99_queue_times.append(f\"{0:.3f}\")\n",
    "            mean_ttfts.append(f\"{0:.3f}\")\n",
    "            p99_ttfts.append(f\"{0:.3f}\")\n",
    "        step_filepath=os.path.join(directory, f\"step_profiling_sharegpt_8192_{qps}_qps_{model_}_tensor_parallelism_{tp_degree}_max_requests_per_batch_256_max_tokens_per_batch_256_num_kv_cache_slots_{kv_cache_slots}_qps_0.000000_num_warmup_requests_0.csv\")\n",
    "        if os.path.exists(step_filepath):\n",
    "            df = pd.read_csv(step_filepath)\n",
    "            ft_throughput = get_ft_throughput(df)\n",
    "            ft_throughputs.append(f\"{ft_throughput:.3f}\")\n",
    "        else:\n",
    "            print(f\"File {step_filepath} does not exist.\")\n",
    "            ft_throughputs.append(f\"{ft_throughput:.3f}\")\n",
    "    print(f\"Model: {model}, TP Degree: {tp_degree}, KV Cache Slots: {kv_cache_slots}\")\n",
    "    print(f\"QPS: {qps_values}\")\n",
    "    print(f\"TPOT - Mean: {mean_tpots}, P99: {p99_tpots}\")\n",
    "    print(f\"Inf Throughput: {inf_throughputs}\")\n",
    "    print(f\"Queueing Time - Mean: {mean_queue_times}, P99: {p99_queue_times}\")\n",
    "    print(f\"TTFT - Mean: {mean_ttfts}, P99: {p99_ttfts}\")\n",
    "    print(f\"Finetuning Throughput: {ft_throughputs}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLAMA-Factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8B model: 3810.46/2 = 1,905.23 tok/s\n",
    "\n",
    "14B model: 1048.954/2 = 524.477 tok/s\n",
    "\n",
    "32B model: 469.974/2 = 234 tok/s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct\n",
      "QPS: ['2.0', '4.0', '6.0', '8.0', '10.0', '12.0', '16.0', '20.0']\n",
      "TPOT - Mean: ['12.986', '15.489', '18.313', '21.861', '29.843', '39.383', '38.819', '38.527'], P99: ['16.120', '21.730', '27.480', '32.325', '42.202', '47.474', '46.787', '46.994']\n",
      "Inf Throughput: ['524.688', '1074.685', '1609.861', '2149.054', '2659.603', '2835.174', '2929.336', '3004.973']\n",
      "TTFT - Mean: ['77.664', '126.601', '189.721', '292.819', '1381.504', '28487.625', '60710.476', '75967.820'], P99: ['378.144', '559.422', '953.589', '1369.075', '7167.328', '56380.458', '139531.039', '182959.385']\n",
      "--------------------------------------------------\n",
      "Model: Qwen/Qwen2.5-14B-Instruct\n",
      "QPS: ['2.0', '4.0', '6.0', '8.0', '10.0', '12.0', '16.0', '20.0']\n",
      "TPOT - Mean: ['15.922', '18.594', '23.196', '36.465', '48.333', '49.917', '49.695', '49.282'], P99: ['20.015', '26.887', '36.449', '53.607', '58.592', '59.535', '59.593', '59.558']\n",
      "Inf Throughput: ['526.911', '1080.292', '1615.444', '2159.424', '2375.092', '2354.388', '2408.425', '2465.998']\n",
      "TTFT - Mean: ['91.587', '155.579', '262.246', '1700.150', '35869.942', '71972.556', '104004.076', '117319.353'], P99: ['517.410', '773.459', '1375.378', '10139.514', '62691.520', '147948.623', '236629.890', '277841.930']\n",
      "--------------------------------------------------\n",
      "Model: Qwen/Qwen2.5-32B-Instruct\n",
      "QPS: ['2.0', '4.0', '6.0', '8.0', '10.0', '12.0', '16.0', '20.0']\n",
      "TPOT - Mean: ['19.499', '23.513', '27.819', '40.453', '48.853', '51.235', '50.966', '50.867'], P99: ['23.649', '32.608', '40.790', '52.860', '58.378', '59.950', '59.612', '59.558']\n",
      "Inf Throughput: ['524.731', '1076.214', '1607.251', '2148.608', '2365.176', '2314.953', '2372.899', '2423.921']\n",
      "TTFT - Mean: ['106.900', '178.247', '308.356', '2223.026', '35940.543', '73707.705', '106031.814', '119248.641'], P99: ['671.845', '827.348', '1575.875', '11781.593', '62096.187', '151155.938', '239800.428', '281852.582']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "directory = \"/global/homes/g/goliaro/flexllm/benchmarking/output/vllm\"\n",
    "models=[\"meta-llama/Llama-3.1-8B-Instruct\", \"Qwen/Qwen2.5-14B-Instruct\", \"Qwen/Qwen2.5-32B-Instruct\"]\n",
    "qps_values=[\"2.0\",\"4.0\",\"6.0\",\"8.0\",\"10.0\",\"12.0\",\"16.0\",\"20.0\"]\n",
    "for i, model in enumerate(models):\n",
    "    model_ = model.replace(\"/\", \"_\").lower()\n",
    "    mean_tpots=[]\n",
    "    p99_tpots=[]\n",
    "    inf_throughputs=[]\n",
    "    mean_ttfts=[]\n",
    "    p99_ttfts=[]\n",
    "    ft_throughputs=[]\n",
    "    for qps in qps_values:\n",
    "        filepath= os.path.join(directory, f\"results_sharegpt_eager_v1_{model_}_bz_256_max_num_batched_tokens_256_{qps}_qps.json\")\n",
    "        if not os.path.exists(filepath):\n",
    "            filepath = os.path.join(directory, f\"results_sharegpt_eager_v1_{model_}_bz_256_max_num_batched_tokens_256_{qps}_qps_.json\")\n",
    "        if os.path.exists(filepath):\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            mean_tpots.append(f\"{data['mean_tpot_ms']:.3f}\")\n",
    "            p99_tpots.append(f\"{data['p99_tpot_ms']:.3f}\")\n",
    "            inf_throughputs.append(f\"{data['output_throughput']:.3f}\")\n",
    "            mean_ttfts.append(f\"{data['mean_ttft_ms']:.3f}\")\n",
    "            p99_ttfts.append(f\"{data['p99_ttft_ms']:.3f}\")\n",
    "        else:\n",
    "            print(f\"File {filepath} does not exist.\")\n",
    "            mean_tpots.append(f\"{0:.3f}\")\n",
    "            p99_tpots.append(f\"{0:.3f}\")\n",
    "            inf_throughputs.append(f\"{0:.3f}\")\n",
    "            mean_ttfts.append(f\"{0:.3f}\")\n",
    "            p99_ttfts.append(f\"{0:.3f}\")\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"QPS: {qps_values}\")\n",
    "    print(f\"TPOT - Mean: {mean_tpots}, P99: {p99_tpots}\")\n",
    "    print(f\"Inf Throughput: {inf_throughputs}\")\n",
    "    print(f\"TTFT - Mean: {mean_ttfts}, P99: {p99_ttfts}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, TP Degree: 1, KV Cache Slots: 70000\n",
      "QPS: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "TPOT - Mean: ['643.117', '640.100', '639.872', '641.125', '642.798'], P99: ['731.466', '712.521', '719.272', '718.240', '733.149']\n",
      "Inf Throughput: ['132.148', '154.654', '156.939', '157.898', '157.601']\n",
      "Queueing Time - Mean: ['74469.599', '374115.479', '422726.020', '451226.948', '468695.820'], P99: ['201956.365', '801683.773', '946641.186', '1014430.762', '1052682.907']\n",
      "TTFT - Mean: ['1120.833', '1092.522', '1112.511', '1119.881', '1124.694'], P99: ['10846.146', '13594.622', '13668.549', '13461.397', '13007.038']\n",
      "Finetuning Throughput: ['5648.021', '5652.325', '5649.419', '5645.506', '5638.343']\n",
      "--------------------------------------------------\n",
      "Model: Qwen/Qwen2.5-14B-Instruct, TP Degree: 2, KV Cache Slots: 70000\n",
      "QPS: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "TPOT - Mean: ['841.062', '840.547', '841.311', '838.705', '842.178'], P99: ['931.530', '949.894', '948.095', '931.680', '934.802']\n",
      "Inf Throughput: ['100.496', '117.284', '118.735', '118.828', '118.332']\n",
      "Queueing Time - Mean: ['153021.671', '553416.400', '600293.155', '635726.953', '658045.620'], P99: ['404252.849', '1216276.904', '1358560.936', '1434390.673', '1480625.647']\n",
      "TTFT - Mean: ['1499.184', '1485.286', '1489.402', '1491.254', '1497.049'], P99: ['14260.693', '17406.797', '17269.157', '17463.007', '17543.671']\n",
      "Finetuning Throughput: ['4379.710', '4376.360', '4369.556', '4377.265', '4359.223']\n",
      "--------------------------------------------------\n",
      "Model: Qwen/Qwen2.5-32B-Instruct, TP Degree: 4, KV Cache Slots: 60000\n",
      "QPS: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "TPOT - Mean: ['848.609', '848.165', '849.264', '848.388', '850.070'], P99: ['941.699', '958.957', '946.155', '945.055', '946.867']\n",
      "Inf Throughput: ['94.674', '107.275', '107.701', '107.809', '107.587']\n",
      "Queueing Time - Mean: ['184490.009', '636940.493', '701974.011', '737552.716', '752896.726'], P99: ['541037.768', '1437795.789', '1599018.953', '1672747.719', '1709979.380']\n",
      "TTFT - Mean: ['1400.624', '1341.124', '1352.497', '1351.606', '1353.389'], P99: ['13689.176', '15041.416', '15746.007', '15733.079', '15769.587']\n",
      "Finetuning Throughput: ['4324.109', '4335.489', '4325.284', '4330.145', '4322.026']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "directory = \"/global/homes/g/goliaro/flexllm/benchmarking/output/e2e/temporal_sharing/profiling\"\n",
    "models=[\"meta-llama/Llama-3.1-8B-Instruct\", \"Qwen/Qwen2.5-14B-Instruct\", \"Qwen/Qwen2.5-32B-Instruct\"]\n",
    "tp_degrees=[1, 2, 4]\n",
    "kv_cache_slots_values=[70000, 70000, 60000]\n",
    "qps_values=[\"1.0\",\"2.0\",\"3.0\",\"4.0\",\"5.0\"]\n",
    "for i, model in enumerate(models):\n",
    "    model_ = model.replace(\"/\", \"_\").lower()\n",
    "    tp_degree = tp_degrees[i]\n",
    "    kv_cache_slots = kv_cache_slots_values[i]\n",
    "    mean_tpots=[]\n",
    "    p99_tpots=[]\n",
    "    inf_throughputs=[]\n",
    "    mean_queue_times=[]\n",
    "    p99_queue_times=[]\n",
    "    mean_ttfts=[]\n",
    "    p99_ttfts=[]\n",
    "    ft_throughputs=[]\n",
    "    for qps in qps_values:\n",
    "        filepath= os.path.join(directory, f\"inference_request_profiling_sharegpt_8192_{qps}_qps_{model_}_tensor_parallelism_{tp_degree}_max_requests_per_batch_256_max_tokens_per_batch_256_num_kv_cache_slots_{kv_cache_slots}_qps_0.000000_num_warmup_requests_0.csv\")\n",
    "        if os.path.exists(filepath):\n",
    "            df = pd.read_csv(filepath)\n",
    "            tpot_mean, tpot_median, tpot_p99 = get_tpot(df)\n",
    "            throughput = get_throughput(df)\n",
    "            queueing_time = get_queueing_time(df)\n",
    "            ttft = get_ttft(df)\n",
    "            mean_tpots.append(f\"{tpot_mean:.3f}\")\n",
    "            p99_tpots.append(f\"{tpot_p99:.3f}\")\n",
    "            inf_throughputs.append(f\"{throughput:.3f}\")\n",
    "            mean_queue_times.append(f\"{queueing_time[0]:.3f}\")\n",
    "            p99_queue_times.append(f\"{queueing_time[2]:.3f}\")\n",
    "            mean_ttfts.append(f\"{ttft[0]:.3f}\")\n",
    "            p99_ttfts.append(f\"{ttft[2]:.3f}\")\n",
    "        else:\n",
    "            print(f\"File {filepath} does not exist.\")\n",
    "            mean_tpots.append(f\"{0:.3f}\")\n",
    "            p99_tpots.append(f\"{0:.3f}\")\n",
    "            inf_throughputs.append(f\"{0:.3f}\")\n",
    "            mean_queue_times.append(f\"{0:.3f}\")\n",
    "            p99_queue_times.append(f\"{0:.3f}\")\n",
    "            mean_ttfts.append(f\"{0:.3f}\")\n",
    "            p99_ttfts.append(f\"{0:.3f}\")\n",
    "        step_filepath=os.path.join(directory, f\"step_profiling_sharegpt_8192_{qps}_qps_{model_}_tensor_parallelism_{tp_degree}_max_requests_per_batch_256_max_tokens_per_batch_256_num_kv_cache_slots_{kv_cache_slots}_qps_0.000000_num_warmup_requests_0.csv\")\n",
    "        if os.path.exists(step_filepath):\n",
    "            df = pd.read_csv(step_filepath)\n",
    "            ft_throughput = get_ft_throughput(df)\n",
    "            ft_throughputs.append(f\"{ft_throughput:.3f}\")\n",
    "        else:\n",
    "            print(f\"File {step_filepath} does not exist.\")\n",
    "            ft_throughputs.append(f\"{0:.3f}\")\n",
    "    print(f\"Model: {model}, TP Degree: {tp_degree}, KV Cache Slots: {kv_cache_slots}\")\n",
    "    print(f\"QPS: {qps_values}\")\n",
    "    print(f\"TPOT - Mean: {mean_tpots}, P99: {p99_tpots}\")\n",
    "    print(f\"Inf Throughput: {inf_throughputs}\")\n",
    "    print(f\"Queueing Time - Mean: {mean_queue_times}, P99: {p99_queue_times}\")\n",
    "    print(f\"TTFT - Mean: {mean_ttfts}, P99: {p99_ttfts}\")\n",
    "    print(f\"Finetuning Throughput: {ft_throughputs}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /global/homes/g/goliaro/flexllm/benchmarking/output/e2e/spatial_sharing/profiling/inference_request_profiling_sharegpt_8192_2.0_qps_meta-llama_llama-3.1-8b-instruct_tensor_parallelism_1_max_requests_per_batch_256_max_tokens_per_batch_256_num_kv_cache_slots_70000_qps_0.000000_num_warmup_requests_0.csv does not exist.\n",
      "File /global/homes/g/goliaro/flexllm/benchmarking/output/e2e/spatial_sharing/profiling/step_profiling_sharegpt_8192_2.0_qps_meta-llama_llama-3.1-8b-instruct_tensor_parallelism_1_max_requests_per_batch_256_max_tokens_per_batch_256_num_kv_cache_slots_70000_qps_0.000000_num_warmup_requests_0.csv does not exist.\n",
      "Model: meta-llama/Llama-3.1-8B-Instruct, TP Degree: 1, KV Cache Slots: 70000\n",
      "QPS: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "TPOT - Mean: ['328.604', '0.000', '336.463', '336.421', '335.331'], P99: ['396.033', '0.000', '404.990', '398.794', '389.157']\n",
      "Inf Throughput: ['198.467', '0.000', '155.932', '296.234', '299.148']\n",
      "Finetuning Throughput: ['5495.135', '0.000', '5522.656', '5401.817', '5392.629']\n",
      "Queueing Time - Mean: ['1740.042', '0.000', '328730.299', '185500.442', '195345.666'], P99: ['12974.293', '0.000', '682632.196', '434073.113', '462454.265']\n",
      "TTFT - Mean: ['769.255', '0.000', '942.521', '930.575', '915.217'], P99: ['5445.626', '0.000', '7317.143', '7182.127', '7704.380']\n",
      "--------------------------------------------------\n",
      "Model: Qwen/Qwen2.5-14B-Instruct, TP Degree: 2, KV Cache Slots: 70000\n",
      "QPS: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "TPOT - Mean: ['423.637', '420.613', '421.271', '422.742', '421.553'], P99: ['483.207', '482.026', '492.725', '484.806', '479.899']\n",
      "Inf Throughput: ['179.701', '228.218', '231.589', '236.177', '236.799']\n",
      "Finetuning Throughput: ['4389.376', '4357.558', '4360.373', '4348.707', '4356.883']\n",
      "Queueing Time - Mean: ['3528.237', '184284.478', '238350.018', '258691.640', '269924.753'], P99: ['16250.383', '400446.189', '549945.281', '609899.611', '639516.285']\n",
      "TTFT - Mean: ['1066.825', '1129.198', '1144.683', '1127.393', '1119.347'], P99: ['7674.555', '9420.985', '8534.804', '8826.099', '8998.645']\n",
      "--------------------------------------------------\n",
      "Model: Qwen/Qwen2.5-32B-Instruct, TP Degree: 4, KV Cache Slots: 60000\n",
      "QPS: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "TPOT - Mean: ['437.324', '433.826', '433.364', '433.560', '432.437'], P99: ['499.864', '512.946', '504.219', '506.025', '503.715']\n",
      "Inf Throughput: ['174.983', '204.994', '208.972', '211.498', '211.558']\n",
      "Finetuning Throughput: ['4268.411', '4258.357', '4266.044', '4259.682', '4255.276']\n",
      "Queueing Time - Mean: ['8474.098', '233405.797', '286669.926', '308460.528', '332058.233'], P99: ['59592.513', '530947.175', '672668.106', '735101.999', '777925.739']\n",
      "TTFT - Mean: ['1093.498', '1092.914', '1127.967', '1086.714', '1094.858'], P99: ['7627.315', '8893.886', '8441.664', '7895.058', '8137.093']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "directory = \"/global/homes/g/goliaro/flexllm/benchmarking/output/e2e/spatial_sharing/profiling\"\n",
    "models=[\"meta-llama/Llama-3.1-8B-Instruct\", \"Qwen/Qwen2.5-14B-Instruct\", \"Qwen/Qwen2.5-32B-Instruct\"]\n",
    "tp_degrees=[1, 2, 4]\n",
    "kv_cache_slots_values=[70000, 70000, 60000]\n",
    "qps_values=[\"1.0\",\"2.0\",\"3.0\",\"4.0\",\"5.0\"]\n",
    "for i, model in enumerate(models):\n",
    "    model_ = model.replace(\"/\", \"_\").lower()\n",
    "    tp_degree = tp_degrees[i]\n",
    "    kv_cache_slots = kv_cache_slots_values[i]\n",
    "    mean_tpots=[]\n",
    "    p99_tpots=[]\n",
    "    inf_throughputs=[]\n",
    "    mean_queue_times=[]\n",
    "    p99_queue_times=[]\n",
    "    mean_ttfts=[]\n",
    "    p99_ttfts=[]\n",
    "    ft_throughputs=[]\n",
    "    for qps in qps_values:\n",
    "        filepath= os.path.join(directory, f\"inference_request_profiling_sharegpt_8192_{qps}_qps_{model_}_tensor_parallelism_{tp_degree}_max_requests_per_batch_256_max_tokens_per_batch_256_num_kv_cache_slots_{kv_cache_slots}_qps_0.000000_num_warmup_requests_0.csv\")\n",
    "        if os.path.exists(filepath):\n",
    "            df = pd.read_csv(filepath)\n",
    "            tpot_mean, tpot_median, tpot_p99 = get_tpot(df)\n",
    "            throughput = get_throughput(df)\n",
    "            queueing_time = get_queueing_time(df)\n",
    "            ttft = get_ttft(df)\n",
    "            mean_tpots.append(f\"{tpot_mean:.3f}\")\n",
    "            p99_tpots.append(f\"{tpot_p99:.3f}\")\n",
    "            inf_throughputs.append(f\"{throughput:.3f}\")\n",
    "            mean_queue_times.append(f\"{queueing_time[0]:.3f}\")\n",
    "            p99_queue_times.append(f\"{queueing_time[2]:.3f}\")\n",
    "            mean_ttfts.append(f\"{ttft[0]:.3f}\")\n",
    "            p99_ttfts.append(f\"{ttft[2]:.3f}\")\n",
    "        else:\n",
    "            print(f\"File {filepath} does not exist.\")\n",
    "            mean_tpots.append(f\"{0:.3f}\")\n",
    "            p99_tpots.append(f\"{0:.3f}\")\n",
    "            inf_throughputs.append(f\"{0:.3f}\")\n",
    "            mean_queue_times.append(f\"{0:.3f}\")\n",
    "            p99_queue_times.append(f\"{0:.3f}\")\n",
    "            mean_ttfts.append(f\"{0:.3f}\")\n",
    "            p99_ttfts.append(f\"{0:.3f}\")\n",
    "        step_filepath=os.path.join(directory, f\"step_profiling_sharegpt_8192_{qps}_qps_{model_}_tensor_parallelism_{tp_degree}_max_requests_per_batch_256_max_tokens_per_batch_256_num_kv_cache_slots_{kv_cache_slots}_qps_0.000000_num_warmup_requests_0.csv\")\n",
    "        if os.path.exists(step_filepath):\n",
    "            df = pd.read_csv(step_filepath)\n",
    "            ft_throughput = get_ft_throughput(df)\n",
    "            ft_throughputs.append(f\"{ft_throughput:.3f}\")\n",
    "        else:\n",
    "            print(f\"File {step_filepath} does not exist.\")\n",
    "            ft_throughputs.append(f\"{0:.3f}\")\n",
    "    print(f\"Model: {model}, TP Degree: {tp_degree}, KV Cache Slots: {kv_cache_slots}\")\n",
    "    print(f\"QPS: {qps_values}\")\n",
    "    print(f\"TPOT - Mean: {mean_tpots}, P99: {p99_tpots}\")\n",
    "    print(f\"Inf Throughput: {inf_throughputs}\")\n",
    "    print(f\"Finetuning Throughput: {ft_throughputs}\")\n",
    "    print(f\"Queueing Time - Mean: {mean_queue_times}, P99: {p99_queue_times}\")\n",
    "    print(f\"TTFT - Mean: {mean_ttfts}, P99: {p99_ttfts}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with both tokens zero: 3716 of 5543 (67.04%)\n"
     ]
    }
   ],
   "source": [
    "fp=\"/global/homes/g/goliaro/flexllm/benchmarking/output/e2e/temporal_sharing/profiling/step_profiling_sharegpt_8192_1.0_qps_meta-llama_llama-3.1-8b-instruct_tensor_parallelism_1_max_requests_per_batch_256_max_tokens_per_batch_256_num_kv_cache_slots_70000_qps_0.000000_num_warmup_requests_0.csv\"\n",
    "df_temporal = pd.read_csv(fp)\n",
    "# count how many rows have both num_prefilling_tokens==0 and num_decoding_tokens==0\n",
    "mask = (df_temporal[\"num_prefilling_tokens\"] == 0) & (df_temporal[\"num_decoding_tokens\"] == 0)\n",
    "count = mask.sum()\n",
    "total = len(df_temporal)\n",
    "pct = count / total * 100\n",
    "print(f\"Rows with both tokens zero: {count} of {total} ({pct:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Sharing (limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, TP Degree: 1, KV Cache Slots: 70000\n",
      "QPS: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "TPOT - Mean: ['35.994', '36.126', '37.553', '38.689', '39.309'], P99: ['46.417', '46.933', '48.007', '51.054', '49.291']\n",
      "Inf Throughput: ['259.261', '513.103', '730.142', '985.756', '1085.179']\n",
      "Finetuning Throughput: ['2108.519', '2040.275', '1925.944', '1786.629', '1690.768']\n",
      "Queueing Time - Mean: ['41.999', '76.036', '106.728', '147.297', '198.028'], P99: ['376.811', '702.075', '1048.795', '1174.243', '1337.289']\n",
      "TTFT - Mean: ['71.663', '77.403', '81.689', '86.242', '87.656'], P99: ['446.426', '573.454', '519.176', '601.988', '590.755']\n",
      "--------------------------------------------------\n",
      "Model: Qwen/Qwen2.5-14B-Instruct, TP Degree: 2, KV Cache Slots: 70000\n",
      "QPS: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "TPOT - Mean: ['61.578', '65.044', '68.632', '69.282', '69.205'], P99: ['73.073', '76.011', '77.754', '78.423', '77.526']\n",
      "Inf Throughput: ['258.217', '503.114', '588.029', '757.054', '821.422']\n",
      "Finetuning Throughput: ['885.568', '801.788', '765.568', '693.829', '641.426']\n",
      "Queueing Time - Mean: ['69.755', '142.952', '257.083', '417.715', '1760.721'], P99: ['738.856', '1303.680', '2259.319', '2905.430', '7529.233']\n",
      "TTFT - Mean: ['122.264', '138.731', '154.055', '160.213', '176.504'], P99: ['766.970', '981.748', '1111.782', '1227.103', '1366.489']\n",
      "--------------------------------------------------\n",
      "Model: Qwen/Qwen2.5-32B-Instruct, TP Degree: 4, KV Cache Slots: 60000\n",
      "QPS: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "TPOT - Mean: ['57.655', '58.728', '60.562', '61.848', '62.945'], P99: ['69.118', '71.396', '72.165', '72.566', '74.308']\n",
      "Inf Throughput: ['261.527', '511.040', '619.357', '801.316', '866.226']\n",
      "Finetuning Throughput: ['761.597', '713.059', '680.187', '635.491', '584.526']\n",
      "Queueing Time - Mean: ['123.529', '220.328', '296.038', '539.269', '3236.451'], P99: ['1054.569', '5665.381', '4130.823', '5822.080', '12944.161']\n",
      "TTFT - Mean: ['116.424', '123.333', '134.949', '143.865', '152.891'], P99: ['671.385', '796.274', '937.744', '1051.905', '1124.554']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "directory = \"/global/homes/g/goliaro/flexllm/benchmarking/output/e2e/spatial_sharing_limited/profiling\"\n",
    "models=[\"meta-llama/Llama-3.1-8B-Instruct\", \"Qwen/Qwen2.5-14B-Instruct\", \"Qwen/Qwen2.5-32B-Instruct\"]\n",
    "tp_degrees=[1, 2, 4]\n",
    "kv_cache_slots_values=[70000, 70000, 60000]\n",
    "qps_values=[\"1.0\",\"2.0\",\"3.0\",\"4.0\",\"5.0\"]\n",
    "for i, model in enumerate(models):\n",
    "    model_ = model.replace(\"/\", \"_\").lower()\n",
    "    tp_degree = tp_degrees[i]\n",
    "    kv_cache_slots = kv_cache_slots_values[i]\n",
    "    mean_tpots=[]\n",
    "    p99_tpots=[]\n",
    "    inf_throughputs=[]\n",
    "    mean_queue_times=[]\n",
    "    p99_queue_times=[]\n",
    "    mean_ttfts=[]\n",
    "    p99_ttfts=[]\n",
    "    ft_throughputs=[]\n",
    "    for qps in qps_values:\n",
    "        filepath= os.path.join(directory, f\"inference_request_profiling_sharegpt_8192_{qps}_qps_{model_}_tensor_parallelism_{tp_degree}_max_requests_per_batch_256_max_tokens_per_batch_256_num_kv_cache_slots_{kv_cache_slots}_qps_0.000000_num_warmup_requests_0.csv\")\n",
    "        if os.path.exists(filepath):\n",
    "            df = pd.read_csv(filepath)\n",
    "            tpot_mean, tpot_median, tpot_p99 = get_tpot(df)\n",
    "            throughput = get_throughput(df)\n",
    "            queueing_time = get_queueing_time(df)\n",
    "            ttft = get_ttft(df)\n",
    "            mean_tpots.append(f\"{tpot_mean:.3f}\")\n",
    "            p99_tpots.append(f\"{tpot_p99:.3f}\")\n",
    "            inf_throughputs.append(f\"{throughput:.3f}\")\n",
    "            mean_queue_times.append(f\"{queueing_time[0]:.3f}\")\n",
    "            p99_queue_times.append(f\"{queueing_time[2]:.3f}\")\n",
    "            mean_ttfts.append(f\"{ttft[0]:.3f}\")\n",
    "            p99_ttfts.append(f\"{ttft[2]:.3f}\")\n",
    "        else:\n",
    "            print(f\"File {filepath} does not exist.\")\n",
    "            mean_tpots.append(f\"{0:.3f}\")\n",
    "            p99_tpots.append(f\"{0:.3f}\")\n",
    "            inf_throughputs.append(f\"{0:.3f}\")\n",
    "            mean_queue_times.append(f\"{0:.3f}\")\n",
    "            p99_queue_times.append(f\"{0:.3f}\")\n",
    "            mean_ttfts.append(f\"{0:.3f}\")\n",
    "            p99_ttfts.append(f\"{0:.3f}\")\n",
    "        step_filepath=os.path.join(directory, f\"step_profiling_sharegpt_8192_{qps}_qps_{model_}_tensor_parallelism_{tp_degree}_max_requests_per_batch_256_max_tokens_per_batch_256_num_kv_cache_slots_{kv_cache_slots}_qps_0.000000_num_warmup_requests_0.csv\")\n",
    "        if os.path.exists(step_filepath):\n",
    "            df = pd.read_csv(step_filepath)\n",
    "            ft_throughput = get_ft_throughput(df)\n",
    "            ft_throughputs.append(f\"{ft_throughput:.3f}\")\n",
    "        else:\n",
    "            print(f\"File {step_filepath} does not exist.\")\n",
    "            ft_throughputs.append(f\"{0:.3f}\")\n",
    "    print(f\"Model: {model}, TP Degree: {tp_degree}, KV Cache Slots: {kv_cache_slots}\")\n",
    "    print(f\"QPS: {qps_values}\")\n",
    "    print(f\"TPOT - Mean: {mean_tpots}, P99: {p99_tpots}\")\n",
    "    print(f\"Inf Throughput: {inf_throughputs}\")\n",
    "    print(f\"Finetuning Throughput: {ft_throughputs}\")\n",
    "    print(f\"Queueing Time - Mean: {mean_queue_times}, P99: {p99_queue_times}\")\n",
    "    print(f\"TTFT - Mean: {mean_ttfts}, P99: {p99_ttfts}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Sharing (limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, TP Degree: 1, KV Cache Slots: 70000\n",
      "QPS: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "TPOT - Mean: ['36.505', '37.869', '39.223', '40.523', '41.344'], P99: ['47.739', '48.968', '50.725', '50.362', '52.389']\n",
      "Inf Throughput: ['258.758', '511.104', '720.476', '964.375', '1060.604']\n",
      "Finetuning Throughput: ['2506.852', '2375.358', '2267.167', '2110.475', '2048.672']\n",
      "Queueing Time - Mean: ['43.129', '79.445', '117.823', '161.835', '240.805'], P99: ['361.234', '650.009', '1136.216', '1259.362', '1661.896']\n",
      "TTFT - Mean: ['70.914', '77.695', '85.546', '88.537', '92.039'], P99: ['473.582', '536.361', '634.297', '629.615', '670.324']\n",
      "--------------------------------------------------\n",
      "Model: Qwen/Qwen2.5-14B-Instruct, TP Degree: 2, KV Cache Slots: 70000\n",
      "QPS: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "TPOT - Mean: ['66.200', '69.697', '74.052', '76.365', '78.048'], P99: ['76.833', '79.636', '84.867', '85.766', '87.552']\n",
      "Inf Throughput: ['256.806', '498.716', '564.383', '726.046', '762.078']\n",
      "Finetuning Throughput: ['999.012', '919.388', '886.912', '812.532', '781.651']\n",
      "Queueing Time - Mean: ['78.051', '166.983', '322.957', '914.976', '15437.594'], P99: ['797.578', '1585.225', '2663.095', '8141.419', '32389.850']\n",
      "TTFT - Mean: ['125.054', '144.595', '164.668', '182.613', '202.649'], P99: ['881.655', '1094.599', '1323.610', '1458.738', '1657.589']\n",
      "--------------------------------------------------\n",
      "Model: Qwen/Qwen2.5-32B-Instruct, TP Degree: 4, KV Cache Slots: 60000\n",
      "QPS: ['1.0', '2.0', '3.0', '4.0', '5.0']\n",
      "TPOT - Mean: ['59.338', '61.479', '63.708', '65.908', '71.982'], P99: ['68.754', '72.109', '74.112', '75.829', '85.862']\n",
      "Inf Throughput: ['261.460', '508.691', '601.976', '787.556', '819.240']\n",
      "Finetuning Throughput: ['875.030', '822.306', '809.788', '747.623', '696.362']\n",
      "Queueing Time - Mean: ['129.907', '210.356', '299.754', '791.097', '18870.555'], P99: ['1020.397', '3618.635', '3085.025', '8425.499', '41590.223']\n",
      "TTFT - Mean: ['116.374', '129.730', '142.679', '154.720', '176.492'], P99: ['751.359', '967.635', '1063.524', '1179.137', '1321.475']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "directory = \"/global/homes/g/goliaro/flexllm/benchmarking/output/e2e/temporal_sharing_limited/profiling\"\n",
    "models=[\"meta-llama/Llama-3.1-8B-Instruct\", \"Qwen/Qwen2.5-14B-Instruct\", \"Qwen/Qwen2.5-32B-Instruct\"]\n",
    "tp_degrees=[1, 2, 4]\n",
    "kv_cache_slots_values=[70000, 70000, 60000]\n",
    "qps_values=[\"1.0\",\"2.0\",\"3.0\",\"4.0\",\"5.0\"]\n",
    "for i, model in enumerate(models):\n",
    "    model_ = model.replace(\"/\", \"_\").lower()\n",
    "    tp_degree = tp_degrees[i]\n",
    "    kv_cache_slots = kv_cache_slots_values[i]\n",
    "    mean_tpots=[]\n",
    "    p99_tpots=[]\n",
    "    inf_throughputs=[]\n",
    "    mean_queue_times=[]\n",
    "    p99_queue_times=[]\n",
    "    mean_ttfts=[]\n",
    "    p99_ttfts=[]\n",
    "    ft_throughputs=[]\n",
    "    for qps in qps_values:\n",
    "        filepath= os.path.join(directory, f\"inference_request_profiling_sharegpt_8192_{qps}_qps_{model_}_tensor_parallelism_{tp_degree}_max_requests_per_batch_256_max_tokens_per_batch_256_num_kv_cache_slots_{kv_cache_slots}_qps_0.000000_num_warmup_requests_0.csv\")\n",
    "        if os.path.exists(filepath):\n",
    "            df = pd.read_csv(filepath)\n",
    "            tpot_mean, tpot_median, tpot_p99 = get_tpot(df)\n",
    "            throughput = get_throughput(df)\n",
    "            queueing_time = get_queueing_time(df)\n",
    "            ttft = get_ttft(df)\n",
    "            mean_tpots.append(f\"{tpot_mean:.3f}\")\n",
    "            p99_tpots.append(f\"{tpot_p99:.3f}\")\n",
    "            inf_throughputs.append(f\"{throughput:.3f}\")\n",
    "            mean_queue_times.append(f\"{queueing_time[0]:.3f}\")\n",
    "            p99_queue_times.append(f\"{queueing_time[2]:.3f}\")\n",
    "            mean_ttfts.append(f\"{ttft[0]:.3f}\")\n",
    "            p99_ttfts.append(f\"{ttft[2]:.3f}\")\n",
    "        else:\n",
    "            print(f\"File {filepath} does not exist.\")\n",
    "            mean_tpots.append(f\"{0:.3f}\")\n",
    "            p99_tpots.append(f\"{0:.3f}\")\n",
    "            inf_throughputs.append(f\"{0:.3f}\")\n",
    "            mean_queue_times.append(f\"{0:.3f}\")\n",
    "            p99_queue_times.append(f\"{0:.3f}\")\n",
    "            mean_ttfts.append(f\"{0:.3f}\")\n",
    "            p99_ttfts.append(f\"{0:.3f}\")\n",
    "        step_filepath=os.path.join(directory, f\"step_profiling_sharegpt_8192_{qps}_qps_{model_}_tensor_parallelism_{tp_degree}_max_requests_per_batch_256_max_tokens_per_batch_256_num_kv_cache_slots_{kv_cache_slots}_qps_0.000000_num_warmup_requests_0.csv\")\n",
    "        if os.path.exists(step_filepath):\n",
    "            df = pd.read_csv(step_filepath)\n",
    "            ft_throughput = get_ft_throughput(df)\n",
    "            ft_throughputs.append(f\"{ft_throughput:.3f}\")\n",
    "        else:\n",
    "            print(f\"File {step_filepath} does not exist.\")\n",
    "            ft_throughputs.append(f\"{0:.3f}\")\n",
    "    print(f\"Model: {model}, TP Degree: {tp_degree}, KV Cache Slots: {kv_cache_slots}\")\n",
    "    print(f\"QPS: {qps_values}\")\n",
    "    print(f\"TPOT - Mean: {mean_tpots}, P99: {p99_tpots}\")\n",
    "    print(f\"Inf Throughput: {inf_throughputs}\")\n",
    "    print(f\"Finetuning Throughput: {ft_throughputs}\")\n",
    "    print(f\"Queueing Time - Mean: {mean_queue_times}, P99: {p99_queue_times}\")\n",
    "    print(f\"TTFT - Mean: {mean_ttfts}, P99: {p99_ttfts}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nersc-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
